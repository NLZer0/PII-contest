{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "# longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = set([\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n",
    "    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n",
    "    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL','O'\n",
    "])\n",
    "\n",
    "label2num = {\n",
    "    'O': 0,\n",
    "    'B-NAME_STUDENT': 1, \n",
    "    'I-NAME_STUDENT': 2, \n",
    "    'B-STREET_ADDRESS': 3, \n",
    "    'I-STREET_ADDRESS': 4, \n",
    "    'B-USERNAME': 5,\n",
    "    'I-USERNAME': 6, \n",
    "    'B-ID_NUM': 7, \n",
    "    'I-ID_NUM': 8, \n",
    "    'B-URL_PERSONAL': 9,\n",
    "    'I-URL_PERSONAL': 10,\n",
    "    'B-EMAIL': 11,\n",
    "    'I-EMAIL': 12,\n",
    "    'B-PHONE_NUM': 13,\n",
    "    'I-PHONE_NUM': 14,\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('saved_models/tokenizer')\n",
    "def tokenize_row(example):\n",
    "    text = []\n",
    "    token_map = []\n",
    "    labels = []\n",
    "    targets = []\n",
    "    idx = 0\n",
    "    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n",
    "        text.append(t)\n",
    "        labels.extend([l]*len(t))\n",
    "        token_map.extend([idx]*len(t))\n",
    "\n",
    "        if l in target_cols:  \n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "        \n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "            token_map.append(-1)\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=2048)  # Adjust max_length if needed\n",
    "    \n",
    "    target_num = sum(targets)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        if start_idx == 0 and end_idx == 0: \n",
    "            token_labels.append(label2num[\"O\"])\n",
    "            continue\n",
    "        \n",
    "        if text[start_idx].isspace():\n",
    "            start_idx += 1\n",
    "        try:\n",
    "            token_labels.append(label2num[labels[start_idx]])\n",
    "        except:\n",
    "            continue\n",
    "    length = len(tokenized.input_ids)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized.input_ids,\n",
    "        \"attention_mask\": tokenized.attention_mask,\n",
    "        \"offset_mapping\": tokenized.offset_mapping,\n",
    "        \"labels\": token_labels,\n",
    "        \"length\": length,\n",
    "        \"target_num\": target_num,\n",
    "        \"group\": 1 if target_num > 0 else 0,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_label_disbalance(data):\n",
    "    label_idxs = {}\n",
    "    for i in range(len(data)):\n",
    "        unique_labels = np.unique(data[i]['labels'])\n",
    "        for lab in unique_labels:\n",
    "            if lab in label_idxs:\n",
    "                label_idxs[lab].append(i)\n",
    "            else:\n",
    "                label_idxs[lab] = [i]\n",
    "\n",
    "    idxs = []\n",
    "    count_o = len(label_idxs[0])\n",
    "    del label_idxs[0]\n",
    "\n",
    "    for it in label_idxs:\n",
    "        scale = count_o // len(label_idxs[it])\n",
    "        idxs += label_idxs[it]*scale\n",
    "\n",
    "    np.random.shuffle(idxs)\n",
    "    data = [data[i] for i in idxs]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding tokens and labels\n",
    "with open('data/mixtral-8x7b-v1.json', 'r', encoding='utf-8') as f:\n",
    "    data_1 = json.load(f)\n",
    "with open('data/train.json', 'r', encoding='utf-8') as f:\n",
    "    data_2 = json.load(f)\n",
    "data = data_1 + data_2\n",
    "\n",
    "tokenized_data = []\n",
    "for doc in data:\n",
    "    row = tokenize_row(doc)\n",
    "    pad_size = 512 - len(row['input_ids'])%512\n",
    "    pad_size = 0 if pad_size%512 == 0 else pad_size\n",
    "\n",
    "    row['input_ids'] = torch.LongTensor(row['input_ids']+[0]*pad_size).reshape(1,-1)\n",
    "    row['attention_mask'] = torch.LongTensor(row['attention_mask']+[0]*pad_size).reshape(1,-1)\n",
    "    row['labels'] = torch.LongTensor(row['labels']+[0]*pad_size)\n",
    "    tokenized_data.append(row)\n",
    "\n",
    "N = len(tokenized_data)\n",
    "train_size = 0.85\n",
    "n = int(N*train_size)\n",
    "train, valid = tokenized_data[:n], tokenized_data[n:]\n",
    "\n",
    "data = None\n",
    "data_tokens = None\n",
    "data_labels = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fix_label_disbalance(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn_module\n",
    "reload(nn_module)\n",
    "from nn_module import BiLSTM_CRF\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = BiLSTM_CRF(embedding_size=768, hidden_size=128, nclasses=len(label2num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d398e1376c7243b3a608e1b35381d839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a98b94b80a47b28c32b860dfbc4110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zmitrovich.nik\\Desktop\\pii_git\\nn_module.py:266\u001b[0m, in \u001b[0;36mBiLSTM_CRF.fit\u001b[1;34m(self, train, nepochs, lr, device)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    265\u001b[0m eploss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_history\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train, \n",
    "    nepochs=10,\n",
    "    lr=1e-3,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': model.optim.state_dict(),\n",
    "    }, 'saved_models/long_bilstm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
